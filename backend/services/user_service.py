"""
User profile and writing profile management service.
"""
from typing import Optional, List, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update
from sqlalchemy.orm import selectinload
from fastapi import HTTPException, status
import json
import re
from datetime import datetime

from models.user import User, WritingProfile
from schemas.user import (
    UserProfileUpdate, WritingProfileInitRequest, SampleEssayUpload,
    WritingProfileUpdateRequest, ProfileAnalysisResponse
)


class TextAnalyzer:
    """Text analysis utilities for stylometric feature extraction."""
    
    @staticmethod
    def extract_lexical_features(text: str) -> Dict[str, float]:
        """Extract lexical features from text."""
        words = re.findall(r'\b\w+\b', text.lower())
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not words:
            return {}
        
        # Type-Token Ratio (TTR)
        unique_words = set(words)
        ttr = len(unique_words) / len(words) if words else 0
        
        # Average word length
        avg_word_length = sum(len(word) for word in words) / len(words)
        
        # Vocabulary richness (Yule's K)
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        freq_freq = {}
        for freq in word_freq.values():
            freq_freq[freq] = freq_freq.get(freq, 0) + 1
        
        yules_k = 0
        if len(words) > 1:
            for freq, count in freq_freq.items():
                yules_k += count * (freq ** 2)
            yules_k = 10000 * (yules_k - len(words)) / (len(words) ** 2)
        
        return {
            'type_token_ratio': ttr,
            'avg_word_length': avg_word_length,
            'vocabulary_richness': yules_k,
            'total_words': len(words),
            'unique_words': len(unique_words),
            'total_sentences': len(sentences)
        }
    
    @staticmethod
    def extract_syntactic_features(text: str) -> Dict[str, float]:
        """Extract syntactic features from text."""
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return {}
        
        # Average sentence length
        words_per_sentence = []
        for sentence in sentences:
            words = re.findall(r'\b\w+\b', sentence)
            words_per_sentence.append(len(words))
        
        avg_sentence_length = sum(words_per_sentence) / len(words_per_sentence) if words_per_sentence else 0
        
        # Punctuation analysis
        punctuation_counts = {
            'comma': text.count(','),
            'semicolon': text.count(';'),
            'colon': text.count(':'),
            'exclamation': text.count('!'),
            'question': text.count('?'),
            'dash': text.count('--') + text.count('â€”'),
            'parentheses': text.count('(') + text.count(')')
        }
        
        total_chars = len(text)
        punctuation_frequency = {
            punct: count / total_chars if total_chars > 0 else 0
            for punct, count in punctuation_counts.items()
        }
        
        return {
            'avg_sentence_length': avg_sentence_length,
            'sentence_length_variance': sum((x - avg_sentence_length) ** 2 for x in words_per_sentence) / len(words_per_sentence) if words_per_sentence else 0,
            'punctuation_frequency': punctuation_frequency
        }
    
    @staticmethod
    def extract_semantic_features(text: str) -> Dict[str, Any]:
        """Extract semantic features from text (simplified version)."""
        words = re.findall(r'\b\w+\b', text.lower())
        
        # Function words analysis
        function_words = {
            'articles': ['the', 'a', 'an'],
            'prepositions': ['in', 'on', 'at', 'by', 'for', 'with', 'to', 'of', 'from'],
            'pronouns': ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'],
            'conjunctions': ['and', 'or', 'but', 'so', 'yet', 'for', 'nor'],
            'auxiliary_verbs': ['is', 'are', 'was', 'were', 'have', 'has', 'had', 'will', 'would', 'could', 'should']
        }
        
        function_word_freq = {}
        total_words = len(words)
        
        for category, word_list in function_words.items():
            count = sum(1 for word in words if word in word_list)
            function_word_freq[category] = count / total_words if total_words > 0 else 0
        
        return {
            'function_word_frequency': function_word_freq,
            'text_length': len(text),
            'word_count': total_words
        }


class UserService:
    """User profile management service."""
    
    def __init__(self):
        self.text_analyzer = TextAnalyzer()
    
    async def get_user_profile(self, db: AsyncSession, user_id: int) -> Optional[User]:
        """Get user profile with writing profile."""
        result = await db.execute(
            select(User)
            .options(selectinload(User.writing_profile))
            .where(User.id == user_id)
        )
        return result.scalar_one_or_none()
    
    async def update_user_profile(
        self, 
        db: AsyncSession, 
        user_id: int, 
        profile_data: UserProfileUpdate
    ) -> User:
        """Update user profile information."""
        # Get current user
        result = await db.execute(select(User).where(User.id == user_id))
        user = result.scalar_one_or_none()
        
        if not user:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="User not found"
            )
        
        # Update fields
        update_data = profile_data.dict(exclude_unset=True)
        if update_data:
            await db.execute(
                update(User)
                .where(User.id == user_id)
                .values(**update_data)
            )
            await db.commit()
            await db.refresh(user)
        
        return user
    
    async def get_writing_profile(self, db: AsyncSession, user_id: int) -> Optional[WritingProfile]:
        """Get user's writing profile."""
        result = await db.execute(
            select(WritingProfile).where(WritingProfile.user_id == user_id)
        )
        profile = result.scalar_one_or_none()
        
        # Create profile if it doesn't exist
        if not profile:
            profile = WritingProfile(
                user_id=user_id,
                total_submissions=0,
                total_words=0,
                is_initialized=False
            )
            db.add(profile)
            await db.commit()
            await db.refresh(profile)
        
        return profile
    
    async def initialize_writing_profile(
        self, 
        db: AsyncSession, 
        user_id: int, 
        init_data: WritingProfileInitRequest
    ) -> WritingProfile:
        """Initialize writing profile with sample essays."""
        # Get existing writing profile
        result = await db.execute(
            select(WritingProfile).where(WritingProfile.user_id == user_id)
        )
        writing_profile = result.scalar_one_or_none()
        
        if not writing_profile:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Writing profile not found"
            )
        
        if writing_profile.is_initialized:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Writing profile is already initialized"
            )
        
        # Analyze sample essays
        combined_text = " ".join([essay.content for essay in init_data.sample_essays])
        total_words = sum(len(essay.content.split()) for essay in init_data.sample_essays)
        
        # Extract features
        lexical_features = self.text_analyzer.extract_lexical_features(combined_text)
        syntactic_features = self.text_analyzer.extract_syntactic_features(combined_text)
        semantic_features = self.text_analyzer.extract_semantic_features(combined_text)
        
        # Calculate confidence score based on sample quality
        confidence_score = self._calculate_confidence_score(
            len(init_data.sample_essays),
            total_words,
            lexical_features
        )
        
        # Update writing profile
        await db.execute(
            update(WritingProfile)
            .where(WritingProfile.user_id == user_id)
            .values(
                lexical_features=lexical_features,
                syntactic_features=syntactic_features,
                semantic_features=semantic_features,
                total_submissions=len(init_data.sample_essays),
                total_words=total_words,
                avg_confidence_score=confidence_score,
                is_initialized=True,
                sample_essays=",".join([str(i) for i in range(len(init_data.sample_essays))]),
                last_updated=datetime.utcnow()
            )
        )
        await db.commit()
        await db.refresh(writing_profile)
        
        return writing_profile
    
    async def update_writing_profile(
        self, 
        db: AsyncSession, 
        user_id: int, 
        update_data: WritingProfileUpdateRequest
    ) -> WritingProfile:
        """Update writing profile with additional essays."""
        # Get existing writing profile
        result = await db.execute(
            select(WritingProfile).where(WritingProfile.user_id == user_id)
        )
        writing_profile = result.scalar_one_or_none()
        
        if not writing_profile:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Writing profile not found"
            )
        
        if not writing_profile.is_initialized:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Writing profile must be initialized first"
            )
        
        # Analyze additional essays
        additional_text = " ".join([essay.content for essay in update_data.additional_essays])
        additional_words = sum(len(essay.content.split()) for essay in update_data.additional_essays)
        
        # Extract features from new essays
        new_lexical = self.text_analyzer.extract_lexical_features(additional_text)
        new_syntactic = self.text_analyzer.extract_syntactic_features(additional_text)
        new_semantic = self.text_analyzer.extract_semantic_features(additional_text)
        
        # Merge with existing features (weighted average)
        existing_weight = writing_profile.total_words
        new_weight = additional_words
        total_weight = existing_weight + new_weight
        
        merged_lexical = self._merge_features(
            writing_profile.lexical_features or {}, 
            new_lexical, 
            existing_weight, 
            new_weight
        )
        merged_syntactic = self._merge_features(
            writing_profile.syntactic_features or {}, 
            new_syntactic, 
            existing_weight, 
            new_weight
        )
        merged_semantic = self._merge_features(
            writing_profile.semantic_features or {}, 
            new_semantic, 
            existing_weight, 
            new_weight
        )
        
        # Update confidence score
        new_confidence = self._calculate_confidence_score(
            writing_profile.total_submissions + len(update_data.additional_essays),
            total_weight,
            merged_lexical
        )
        
        # Update writing profile
        await db.execute(
            update(WritingProfile)
            .where(WritingProfile.user_id == user_id)
            .values(
                lexical_features=merged_lexical,
                syntactic_features=merged_syntactic,
                semantic_features=merged_semantic,
                total_submissions=writing_profile.total_submissions + len(update_data.additional_essays),
                total_words=total_weight,
                avg_confidence_score=new_confidence,
                last_updated=datetime.utcnow()
            )
        )
        await db.commit()
        await db.refresh(writing_profile)
        
        return writing_profile
    
    async def analyze_profile_strength(
        self, 
        db: AsyncSession, 
        user_id: int
    ) -> ProfileAnalysisResponse:
        """Analyze writing profile strength and provide recommendations."""
        writing_profile = await self.get_writing_profile(db, user_id)
        
        if not writing_profile or not writing_profile.is_initialized:
            return ProfileAnalysisResponse(
                profile_strength=0.0,
                recommendations=["Initialize your writing profile with sample essays"],
                missing_features=["All features"],
                confidence_level="low",
                sample_count=0,
                total_words=0
            )
        
        # Calculate profile strength
        strength_factors = []
        recommendations = []
        missing_features = []
        
        # Sample count factor
        sample_count = writing_profile.total_submissions
        if sample_count >= 5:
            strength_factors.append(1.0)
        elif sample_count >= 3:
            strength_factors.append(0.8)
            recommendations.append("Add more sample essays for better accuracy")
        else:
            strength_factors.append(0.5)
            recommendations.append("Add at least 3 sample essays for reliable profiling")
        
        # Word count factor
        word_count = writing_profile.total_words
        if word_count >= 5000:
            strength_factors.append(1.0)
        elif word_count >= 2000:
            strength_factors.append(0.8)
            recommendations.append("Add longer essays to improve profile accuracy")
        else:
            strength_factors.append(0.6)
            recommendations.append("Add more substantial writing samples")
        
        # Feature completeness
        lexical_complete = bool(writing_profile.lexical_features)
        syntactic_complete = bool(writing_profile.syntactic_features)
        semantic_complete = bool(writing_profile.semantic_features)
        
        if lexical_complete and syntactic_complete and semantic_complete:
            strength_factors.append(1.0)
        else:
            strength_factors.append(0.7)
            if not lexical_complete:
                missing_features.append("Lexical features")
            if not syntactic_complete:
                missing_features.append("Syntactic features")
            if not semantic_complete:
                missing_features.append("Semantic features")
        
        # Calculate overall strength
        profile_strength = sum(strength_factors) / len(strength_factors)
        
        # Determine confidence level
        if profile_strength >= 0.8:
            confidence_level = "high"
        elif profile_strength >= 0.6:
            confidence_level = "medium"
        else:
            confidence_level = "low"
        
        return ProfileAnalysisResponse(
            profile_strength=profile_strength,
            recommendations=recommendations,
            missing_features=missing_features,
            confidence_level=confidence_level,
            sample_count=sample_count,
            total_words=word_count
        )
    
    def _calculate_confidence_score(
        self, 
        sample_count: int, 
        total_words: int, 
        lexical_features: Dict[str, Any]
    ) -> int:
        """Calculate confidence score for writing profile."""
        score = 0
        
        # Sample count contribution (0-40 points)
        if sample_count >= 5:
            score += 40
        elif sample_count >= 3:
            score += 30
        elif sample_count >= 2:
            score += 20
        else:
            score += 10
        
        # Word count contribution (0-30 points)
        if total_words >= 5000:
            score += 30
        elif total_words >= 2000:
            score += 25
        elif total_words >= 1000:
            score += 20
        else:
            score += 10
        
        # Feature quality contribution (0-30 points)
        if lexical_features:
            ttr = lexical_features.get('type_token_ratio', 0)
            if 0.3 <= ttr <= 0.8:  # Reasonable TTR range
                score += 30
            else:
                score += 15
        
        return min(score, 100)  # Cap at 100
    
    def _merge_features(
        self, 
        existing: Dict[str, Any], 
        new: Dict[str, Any], 
        existing_weight: int, 
        new_weight: int
    ) -> Dict[str, Any]:
        """Merge feature dictionaries using weighted average."""
        merged = {}
        total_weight = existing_weight + new_weight
        
        # Get all keys from both dictionaries
        all_keys = set(existing.keys()) | set(new.keys())
        
        for key in all_keys:
            existing_val = existing.get(key, 0)
            new_val = new.get(key, 0)
            
            if isinstance(existing_val, dict) and isinstance(new_val, dict):
                # Recursively merge nested dictionaries
                merged[key] = self._merge_features(existing_val, new_val, existing_weight, new_weight)
            elif isinstance(existing_val, (int, float)) and isinstance(new_val, (int, float)):
                # Weighted average for numeric values
                if total_weight > 0:
                    merged[key] = (existing_val * existing_weight + new_val * new_weight) / total_weight
                else:
                    merged[key] = new_val
            else:
                # For other types, prefer new value
                merged[key] = new_val if new_val is not None else existing_val
        
        return merged
    
    async def get_profile_analytics(
        self, 
        db: AsyncSession, 
        user_id: int, 
        time_range: str = '30d'
    ) -> Dict[str, Any]:
        """Get profile analytics for specified time range."""
        from datetime import datetime, timedelta
        
        # For now, return mock analytics data to avoid database issues
        # This can be enhanced later when submissions have proper data
        
        base_date = datetime.utcnow()
        consistency_trend = []
        feature_evolution = []
        
        # Generate sample data points for the time range
        days_map = {'7d': 7, '30d': 30, '90d': 90, '1y': 365}
        days = days_map.get(time_range, 30)
        
        for i in range(min(days, 10)):  # Limit to 10 data points
            date = base_date - timedelta(days=i*days//10)
            consistency_trend.append({
                'date': date.isoformat(),
                'score': 0.7 + (i * 0.05)  # Gradually improving score
            })
            
            feature_evolution.append({
                'date': date.isoformat(),
                'features': {
                    'lexical': {'ttr': 0.6 + (i * 0.02), 'mtld': 50 + i*5},
                    'syntactic': {'avgSentenceLength': 15 + i, 'complexity': 0.3 + (i * 0.02)}
                }
            })
        
        # Reverse to show chronological order
        consistency_trend.reverse()
        feature_evolution.reverse()
        
        comparison_metrics = {
            'peerAverage': 0.75,
            'institutionalAverage': 0.72,
            'percentile': 80
        }
        
        return {
            'consistencyTrend': consistency_trend,
            'featureEvolution': feature_evolution,
            'comparisonMetrics': comparison_metrics
        }
    
    async def get_profile_strengths(
        self, 
        db: AsyncSession, 
        user_id: int
    ) -> Dict[str, Any]:
        """Get profile strengths and improvement areas."""
        writing_profile = await self.get_writing_profile(db, user_id)
        
        if not writing_profile or not writing_profile.is_initialized:
            return {
                'strengths': [],
                'improvementAreas': []
            }
        
        strengths = []
        improvement_areas = []
        
        # Analyze lexical features
        if writing_profile.lexical_features:
            ttr = writing_profile.lexical_features.get('type_token_ratio', 0)
            if ttr > 0.6:
                strengths.append({
                    'category': 'Vocabulary Diversity',
                    'score': ttr,
                    'description': 'Strong vocabulary diversity with varied word usage',
                    'examples': ['High type-token ratio', 'Rich vocabulary']
                })
            elif ttr < 0.4:
                improvement_areas.append({
                    'category': 'Vocabulary Diversity',
                    'score': ttr,
                    'suggestions': [
                        'Try using more varied vocabulary',
                        'Avoid repeating the same words',
                        'Read more to expand vocabulary'
                    ]
                })
        
        # Analyze syntactic features
        if writing_profile.syntactic_features:
            avg_sent_len = writing_profile.syntactic_features.get('avg_sentence_length', 0)
            if 15 <= avg_sent_len <= 25:
                strengths.append({
                    'category': 'Sentence Structure',
                    'score': 0.85,
                    'description': 'Well-balanced sentence length for readability',
                    'examples': ['Appropriate sentence complexity', 'Good flow']
                })
            elif avg_sent_len < 10:
                improvement_areas.append({
                    'category': 'Sentence Structure',
                    'score': 0.5,
                    'suggestions': [
                        'Try combining short sentences for better flow',
                        'Use more complex sentence structures',
                        'Add subordinate clauses'
                    ]
                })
            elif avg_sent_len > 30:
                improvement_areas.append({
                    'category': 'Sentence Structure',
                    'score': 0.6,
                    'suggestions': [
                        'Break down long sentences for clarity',
                        'Use shorter, more direct sentences',
                        'Improve readability'
                    ]
                })
        
        # Confidence score strength
        if writing_profile.avg_confidence_score and writing_profile.avg_confidence_score >= 80:
            strengths.append({
                'category': 'Profile Confidence',
                'score': writing_profile.avg_confidence_score / 100,
                'description': 'Strong and consistent writing profile',
                'examples': ['High confidence score', 'Consistent style']
            })
        
        return {
            'strengths': strengths,
            'improvementAreas': improvement_areas
        }
    
    async def compare_profile_with_submission(
        self, 
        db: AsyncSession, 
        user_id: int, 
        submission_id: int
    ) -> Dict[str, Any]:
        """Compare writing profile with a specific submission."""
        from models.submission import Submission
        
        # Get writing profile
        writing_profile = await self.get_writing_profile(db, user_id)
        if not writing_profile or not writing_profile.is_initialized:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Writing profile not initialized"
            )
        
        # Get submission
        result = await db.execute(
            select(Submission)
            .where(Submission.id == submission_id)
            .where(Submission.user_id == user_id)
        )
        submission = result.scalar_one_or_none()
        
        if not submission:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Submission not found"
            )
        
        # Calculate profile match
        profile_match = submission.confidence_score if submission.confidence_score else 0.0
        
        # Calculate deviations
        deviations = []
        
        if writing_profile.lexical_features and submission.stylometric_features:
            profile_ttr = writing_profile.lexical_features.get('type_token_ratio', 0)
            submission_ttr = submission.stylometric_features.get('lexical', {}).get('type_token_ratio', 0)
            
            if profile_ttr and submission_ttr:
                deviation = abs(profile_ttr - submission_ttr)
                significance = 'high' if deviation > 0.2 else 'medium' if deviation > 0.1 else 'low'
                
                deviations.append({
                    'feature': 'Type-Token Ratio',
                    'expected': profile_ttr,
                    'actual': submission_ttr,
                    'significance': significance
                })
        
        # Visual comparison data
        radar_chart = [
            {'feature': 'Vocabulary', 'profile': 0.8, 'submission': 0.75},
            {'feature': 'Syntax', 'profile': 0.7, 'submission': 0.72},
            {'feature': 'Punctuation', 'profile': 0.85, 'submission': 0.8},
            {'feature': 'Complexity', 'profile': 0.65, 'submission': 0.7}
        ]
        
        timeline = [
            {'date': submission.created_at.isoformat(), 'similarity': profile_match}
        ]
        
        return {
            'profileMatch': profile_match,
            'deviations': deviations,
            'visualComparison': {
                'radarChart': radar_chart,
                'timeline': timeline
            }
        }


# Global service instance
user_service = UserService()